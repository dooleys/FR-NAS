{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85408a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from analysis import *\n",
    "import glob\n",
    "import string\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f06e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_results(results, split, write=False):\n",
    "    mean_acc = reduce(lambda x, y: pd.merge(x, y, on='index'), [x['acc_df'] for x in results if x['split'] == split]).mean(axis=1)\n",
    "    mean_rank = reduce(lambda x, y: pd.merge(x, y, on='index'), [x['disp_df'] for x in results if x['split'] == split]).mean(axis=1)\n",
    "    std_acc = reduce(lambda x, y: pd.merge(x, y, on='index'), [x['acc_df'] for x in results if x['split'] == split]).sem(axis=1)\n",
    "    std_rank = reduce(lambda x, y: pd.merge(x, y, on='index'), [x['disp_df'] for x in results if x['split'] == split]).sem(axis=1)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['Accuracy_mean', 'Accuracy_std', 'Disparity_mean', 'Disparity_std'])\n",
    "    df['Accuracy_mean'] = mean_acc\n",
    "    df['Disparity_mean'] = mean_rank\n",
    "    df['Accuracy_std'] = std_acc\n",
    "    df['Disparity_std'] = std_rank\n",
    "    df['Model'] = [x[:x.find('Face')-4] if 'dpn' not in x else x[:x.find('Face')+4].replace('dpn107','DPN') for x in df.index]\n",
    "    if write:\n",
    "        df.to_csv(f'vggface_phase1_{split}.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2ddb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tables(metric_function_output, **args):\n",
    "    results = []\n",
    "    for split in ['val','test']:\n",
    "        metadata = pd.read_csv(f'vggface2_{split}_identities_gender.csv')\n",
    "        for seed in [111,222,333]:\n",
    "            pickle_files = []\n",
    "            for model in glob.glob(f'vggface2_train_{seed}/[!C]*'):\n",
    "                pickle_files += [glob.glob(model+f'/*[!ema]_{split}.pkl')]\n",
    "\n",
    "            epochs = [f'epoch_{x}' for x in range(11,12)]\n",
    "\n",
    "            acc_df_vgg, _, _ = analyze_pickle_files(pickle_files, metadata, epochs=epochs)\n",
    "            disp_df = analyze_pickle_files(pickle_files, metadata, epochs=epochs, **args)[metric_function_output]\n",
    "            acc_df_vgg['Metric'] = 1 - acc_df_vgg['Metric']\n",
    "\n",
    "            #SMAC models\n",
    "            pickle_files = []\n",
    "            for model in glob.glob(f'vggface2_train_{seed}/Checkpoint*'):\n",
    "                if len([glob.glob(model+f'/*[!ema]_{split}.pkl')][0]):\n",
    "                    pickle_files += [glob.glob(model+f'/*[!ema]_{split}.pkl')]\n",
    "                else:\n",
    "                    pickle_files += [glob.glob(model+f'/*.pkl')]\n",
    "\n",
    "            epochs = [f'epoch_{x}' for x in range(10,11)]\n",
    "\n",
    "            acc_df_vgg_smac, _, _ = analyze_pickle_files(pickle_files, metadata, epochs=epochs)\n",
    "            disp_df_smac = analyze_pickle_files(pickle_files, metadata, epochs=epochs, **args)[metric_function_output]\n",
    "            acc_df_vgg_smac['Metric'] = 1 - acc_df_vgg_smac['Metric']\n",
    "\n",
    "            acc_df_vgg = pd.concat([acc_df_vgg,acc_df_vgg_smac])\n",
    "            disp_df = pd.concat([disp_df,disp_df_smac])\n",
    "            res = {\n",
    "                'split': split,\n",
    "                'seed': seed,\n",
    "                'acc_df': acc_df_vgg[['index','Metric']].set_index('index'),\n",
    "                'disp_df': disp_df[['index','Metric']].set_index('index')\n",
    "                  }\n",
    "            results += [res]\n",
    "    df_val = reduce_results(results, 'val')\n",
    "    df_test = reduce_results(results, 'test')\n",
    "    return df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3905c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure(metric_name, metric_suffix, split, df):\n",
    "    fig = px.scatter(df, \n",
    "                     x='Accuracy_mean', \n",
    "                     y='Disparity_mean', \n",
    "                     error_x = \"Accuracy_std\", \n",
    "                     error_y = \"Disparity_std\", \n",
    "                     color=\"Model\", \n",
    "                     color_discrete_map=color_map,\n",
    "                     template=\"simple_white\",\n",
    "                     width=1200, height= 1000\n",
    "                    )\n",
    "    if metric_name == 'Rank Disparity':\n",
    "        fig.update_layout(\n",
    "            xaxis_range=[0,0.15],\n",
    "            yaxis_range=[-.01,.6]\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Error\",\n",
    "        yaxis_title=metric_name,\n",
    "        legend_title=\"Models\",\n",
    "        font=dict(\n",
    "            family=\"Times New Roman\",\n",
    "            size=38,\n",
    "            color=\"Black\"\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=20))\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "        font=dict(\n",
    "            family=\"Times New Roman\",\n",
    "            size=28,\n",
    "            color=\"Black\"\n",
    "        )\n",
    "    ))\n",
    "    p = np.array(preparePareto(df[['Accuracy_mean','Disparity_mean']], False, False).dropna())\n",
    "    for x, y in zip(p[:-1], p[1:]):\n",
    "        fig.add_shape(type='line',\n",
    "                    x0=x[0],y0=x[1],x1=y[0],y1=y[1],\n",
    "                    line=dict(color='gray',width=4),line_dash='dash',\n",
    "                    xref='x',yref='y')\n",
    "    plotly.io.write_image(fig, f'_RQ2_{split}_vgg_{metric_suffix}.png', format='png')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b90f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#e6194B', '#3cb44b', '#ffe119','#f58231','#42d4f4',\n",
    "          '#f032e6','#fabed4','#469990','#aaffc3','#000075','#e6194B','#9a6324','#dcbeff', '#42d4f4']\n",
    "plotted_models = ['DPN','ReXNet', 'Other', 'TNT', 'Inception', 'HRNet', 'EseVoVNet', 'VGG19', 'ResNet-RS', \n",
    "                 'DenseNet', 'DPN_MagFace', 'DPN_CosFace', 'SMAC', 'Swin_Transformer']\n",
    "color_map = {}\n",
    "for c,m in zip(colors,plotted_models):\n",
    "    color_map[m] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58861036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase2(metric_name, metric_suffix, metric_function_output, plot=False, **args):\n",
    "    lookup = {\n",
    "        'Checkpoints_Edges_301_LR_0.13828312564892567_Head': 'SMAC',\n",
    "        'Checkpoints_Edges_258_LR_0.1404172769842098_Head': 'SMAC',\n",
    "        'Checkpoints_Edges_301_LR_0.13828312564892567_Head': 'SMAC',\n",
    "        'Checkpoints_Edges_248_LR_0.09532880096168164_Head': 'SMAC',\n",
    "        'coat_lite_small': 'Other',\n",
    "        'convit_base': 'Other',\n",
    "        'cspdarknet53': 'Other',\n",
    "        'dla102x2': 'Other',\n",
    "        'DPN_ArcFace': 'Other',\n",
    "        'DPN_MagFace': 'DPN_MagFace',\n",
    "        'DPN_CosFace': 'DPN_CosFace',\n",
    "        'ese_vovnet39b': 'EseVoVNet',\n",
    "        'hrnet_w64': 'HRNet',\n",
    "        'jx_nest_base': 'Other',\n",
    "        'rexnet_200': 'ReXNet',\n",
    "        'swin_base_patch4_window7_224': 'Swin_Transformer',\n",
    "        'tf_efficientnet_b7_ns': 'Other',\n",
    "        'tnt_s_patch16_224': 'TNT',\n",
    "        'twins_svt_large': 'Other'\n",
    "    }\n",
    "    \n",
    "    df_val, df_test = make_tables(metric_function_output, **args)\n",
    "    df_val['Model'] = df_val['Model'].apply(lambda l: lookup[l])\n",
    "    df_test['Model'] = df_test['Model'].apply(lambda l: lookup[l])\n",
    "    if plot:\n",
    "        plot_figure(metric_name, metric_suffix, 'val', df_val)\n",
    "        plot_figure(metric_name, metric_suffix, 'test', df_test)   \n",
    "    return df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49885529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val, df_test = phase2('Rank Disparity', '', 2)\n",
    "# df_val, df_test =  phase2('Disparity','disparity', 1)\n",
    "# df_val, df_test = phase2('Rank Ratio', 'rank_ratio', 2, ratio=True)\n",
    "# df_val, df_test = phase2('Ratio', 'ratio', 1, ratio=True)\n",
    "df_val, df_test = phase2('Error Ratio', 'error_ratio', 1, ratio=True, error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764f72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.to_csv('vggface_phase2pareto_errorratio_val.csv')\n",
    "df_test.to_csv('vggface_phase2pareto_errorratio_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
