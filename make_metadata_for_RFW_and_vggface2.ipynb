{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736ee897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import torchvision.datasets as datasets\n",
    "from src.utils.data_utils_balanced import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bb7ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backbone': 'dpn107', 'batch_size': 64, 'bn_eps': None, 'bn_momentum': None, 'checkpoints_root': 'Checkpoints/timm_explore_few_epochs/', 'clip_grad': None, 'clip_mode': 'norm', 'comet_api_key': 'KKiKMVZI9RCYowoKDZDS5Y2km', 'comet_workspace': 'rsukthanker', 'cooldown_epochs': 10, 'dataset': 'CelebA', 'decay_epochs': 100, 'decay_rate': 0.1, 'default_test_root': '/work/dlclarge2/sukthank-ZCP_Competition/FairNAS/FR-NAS/data/CelebA/Img/img_align_celeba_splits/val/', 'default_train_root': '/work/dlclarge2/sukthank-ZCP_Competition/FairNAS/FR-NAS/data/CelebA/Img/img_align_celeba_splits/train/', 'demographics_file': 'CelebA/CelebA_demographics.txt', 'dist_bn': 'reduce', 'drop': 0.0, 'drop_block': None, 'drop_connect': None, 'drop_path': None, 'epoch_repeats': 0.0, 'epochs': 100, 'file_name': 'timm_from-scratch.csv', 'file_name_ema': 'timm_from-scratch_ema.csv', 'gp': None, 'groups_to_modify': ['male', 'female'], 'head': 'CosFace', 'input_size': 224, 'layer_decay': None, 'lr': 0.001, 'lr_cycle_decay': 0.5, 'lr_cycle_limit': 1, 'lr_cycle_mul': 1.0, 'lr_k_decay': 1.0, 'lr_noise': None, 'lr_noise_pct': 0.67, 'lr_noise_std': 1.0, 'mean': [0.5, 0.5, 0.5], 'metadata_file': 'timm_model_metadata.csv', 'min_lr': 1e-06, 'min_num_images': 3, 'model_ema': False, 'model_ema_decay': 0.9998, 'model_ema_force_cpu': False, 'momentum': 0.9, 'name': 'CelebA', 'num_workers': 4, 'opt': 'sgd', 'opt_betas': None, 'opt_eps': None, 'out_dir': '.', 'p_identities': [1.0, 1.0], 'p_images': [1.0, 1.0], 'patience_epochs': 10, 'pretrained': False, 'project_name': 'from-scratch_no-resampling_adam', 'save_freq': 1, 'sched': 'multistep', 'seed': 222, 'split_bn': False, 'start_epoch': None, 'std': [0.5, 0.5, 0.5], 'sync_bn': False, 'torchscript': False, 'train_loss': 'Focal', 'user_config': 'config_user.yaml', 'warmup_epochs': 3, 'warmup_lr': 0.0001, 'weight_decay': 0.0001}\n",
      "P identities: {'African': 1.0, 'Asian': 1.0, 'Caucasian': 1.0, 'Indian': 1.0}\n",
      "P images: {'African': 1.0, 'Asian': 1.0, 'Caucasian': 1.0, 'Indian': 1.0}\n",
      "PREPARING TRAIN DATASET\n",
      "Overall # of images for African available is 8327\n",
      "# images selected for African is 8327\n",
      "Overall # of images for Asian available is 9322\n",
      "# images selected for Asian is 8327\n",
      "Overall # of images for Caucasian available is 8380\n",
      "# images selected for Caucasian is 8327\n",
      "Overall # of images for Indian available is 8411\n",
      "# images selected for Indian is 8327\n",
      "Number of idx for African is 2309\n",
      "Number of idx for Asian is 2309\n",
      "Number of idx for Caucasian is 2309\n",
      "Number of idx for Indian is 2309\n",
      "PREPARING TEST DATASET\n",
      "Overall # of images for African available is 8327\n",
      "# images selected for African is 8327\n",
      "Overall # of images for Asian available is 9322\n",
      "# images selected for Asian is 8327\n",
      "Overall # of images for Caucasian available is 8380\n",
      "# images selected for Caucasian is 8327\n",
      "Overall # of images for Indian available is 8411\n",
      "# images selected for Indian is 8327\n",
      "Number of idx for African is 2309\n",
      "Number of idx for Asian is 2309\n",
      "Number of idx for Caucasian is 2309\n",
      "Number of idx for Indian is 2309\n",
      "Len of train dataloader is 520\n",
      "Len of test dataloader is 521\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--config_path', type=str)\n",
    "\n",
    "args = parser.parse_args(['--config_path','/cmlscratch/sdooley1/merge_timm/FR-NAS/configs/dpn107/config_dpn107_CosFace_sgd.yaml'])\n",
    "\n",
    "\n",
    "with open(args.config_path, \"r\") as ymlfile:\n",
    "    options = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    print(options)\n",
    "for key, value in options.items():\n",
    "    setattr(args, key, value)\n",
    "\n",
    "# dummy train example\n",
    "args.default_train_root = '/cmlscratch/sdooley1/data/RFW/test/img_split/'\n",
    "args.default_test_root = '/cmlscratch/sdooley1/data/RFW/test/img_split/'\n",
    "args.demographics_file = '/cmlscratch/sdooley1/data/RFW/RFW_demographics.txt'\n",
    "args.groups_to_modify = ['African','Asian','Caucasian','Indian']\n",
    "args.p_images = [1.0,1.0,1.0,1.0]\n",
    "args.p_identities = [1.0,1.0,1.0,1.0]\n",
    "args.RFW_checkpoints_root = 'Checkpoints/RFW/'\n",
    "args.dataset = 'RFW'\n",
    "\n",
    "p_images = {\n",
    "    args.groups_to_modify[i]: args.p_images[i]\n",
    "    for i in range(len(args.groups_to_modify))\n",
    "}\n",
    "p_identities = {\n",
    "    args.groups_to_modify[i]: args.p_identities[i]\n",
    "    for i in range(len(args.groups_to_modify))\n",
    "}\n",
    "args.p_images = p_images\n",
    "args.p_identities = p_identities\n",
    "\n",
    "print(\"P identities: {}\".format(args.p_identities))\n",
    "print(\"P images: {}\".format(args.p_images))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataloaders, num_class, demographic_to_labels_train, demographic_to_labels_test = prepare_data(\n",
    "    args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b37f1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['ids','label','ethnicity'])\n",
    "for _, label, eth, ids in dataloaders['test']:\n",
    "    df = df.append(pd.DataFrame({'ids':ids, 'label': label, 'ethnicity': eth}),\n",
    "                  ignore_index=True)\n",
    "    \n",
    "df.to_csv('Checkpoints/RFW_test_identities_ethnicity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce79b7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backbone': 'dpn107', 'batch_size': 64, 'bn_eps': None, 'bn_momentum': None, 'checkpoints_root': 'Checkpoints/timm_explore_few_epochs/', 'clip_grad': None, 'clip_mode': 'norm', 'comet_api_key': 'KKiKMVZI9RCYowoKDZDS5Y2km', 'comet_workspace': 'rsukthanker', 'cooldown_epochs': 10, 'dataset': 'CelebA', 'decay_epochs': 100, 'decay_rate': 0.1, 'default_test_root': '/work/dlclarge2/sukthank-ZCP_Competition/FairNAS/FR-NAS/data/CelebA/Img/img_align_celeba_splits/val/', 'default_train_root': '/work/dlclarge2/sukthank-ZCP_Competition/FairNAS/FR-NAS/data/CelebA/Img/img_align_celeba_splits/train/', 'demographics_file': 'CelebA/CelebA_demographics.txt', 'dist_bn': 'reduce', 'drop': 0.0, 'drop_block': None, 'drop_connect': None, 'drop_path': None, 'epoch_repeats': 0.0, 'epochs': 100, 'file_name': 'timm_from-scratch.csv', 'file_name_ema': 'timm_from-scratch_ema.csv', 'gp': None, 'groups_to_modify': ['male', 'female'], 'head': 'CosFace', 'input_size': 224, 'layer_decay': None, 'lr': 0.001, 'lr_cycle_decay': 0.5, 'lr_cycle_limit': 1, 'lr_cycle_mul': 1.0, 'lr_k_decay': 1.0, 'lr_noise': None, 'lr_noise_pct': 0.67, 'lr_noise_std': 1.0, 'mean': [0.5, 0.5, 0.5], 'metadata_file': 'timm_model_metadata.csv', 'min_lr': 1e-06, 'min_num_images': 3, 'model_ema': False, 'model_ema_decay': 0.9998, 'model_ema_force_cpu': False, 'momentum': 0.9, 'name': 'CelebA', 'num_workers': 4, 'opt': 'sgd', 'opt_betas': None, 'opt_eps': None, 'out_dir': '.', 'p_identities': [1.0, 1.0], 'p_images': [1.0, 1.0], 'patience_epochs': 10, 'pretrained': False, 'project_name': 'from-scratch_no-resampling_adam', 'save_freq': 1, 'sched': 'multistep', 'seed': 222, 'split_bn': False, 'start_epoch': None, 'std': [0.5, 0.5, 0.5], 'sync_bn': False, 'torchscript': False, 'train_loss': 'Focal', 'user_config': 'config_user.yaml', 'warmup_epochs': 3, 'warmup_lr': 0.0001, 'weight_decay': 0.0001}\n",
      "P identities: {'male': 1.0, 'female': 1.0}\n",
      "P images: {'male': 1.0, 'female': 1.0}\n",
      "PREPARING TRAIN DATASET\n",
      "Overall # of images for male available is 69515\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1125600/4176143054.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m dataloaders, num_class, demographic_to_labels_train, demographic_to_labels_test = prepare_data(\n\u001b[0m\u001b[1;32m     39\u001b[0m     args)\n",
      "\u001b[0;32m/cmlscratch/sdooley1/merge_timm/FR-NAS/src/utils/data_utils_balanced.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PREPARING TRAIN DATASET'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     datasets['train'] = ImageFolderWithProtectedAttributes(args.default_train_root, transform=train_transform,\n\u001b[0m\u001b[1;32m    343\u001b[0m                                                                  \u001b[0mdemographic_to_all_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdemographic_to_all_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                                                                  \u001b[0mall_classes_to_demographic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_classes_to_demographic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/sdooley1/merge_timm/FR-NAS/src/utils/data_utils_balanced.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, demographic_to_all_classes, all_classes_to_demographic, p_identities, p_images, min_num, ref_num_images, seed)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_classes_to_demographic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_num_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cmlscratch/sdooley1/merge_timm/FR-NAS/src/utils/data_utils_balanced.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, p_images, all_classes_to_demographic, min_num, ref_num_images, seed)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Overall # of images for {} available is {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_essential\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minstances_additional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0minstances_additional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_additional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_additional_images_to_keep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0minstances_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstances_essential\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minstances_additional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# images selected for {} is {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--config_path', type=str)\n",
    "\n",
    "args = parser.parse_args(['--config_path','/cmlscratch/sdooley1/merge_timm/FR-NAS/configs/dpn107/config_dpn107_CosFace_sgd.yaml'])\n",
    "\n",
    "\n",
    "with open(args.config_path, \"r\") as ymlfile:\n",
    "    options = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    print(options)\n",
    "for key, value in options.items():\n",
    "    setattr(args, key, value)\n",
    "\n",
    "# dummy train example\n",
    "args.default_train_root = '/cmlscratch/sdooley1/data/vggface2/test/'\n",
    "args.default_test_root = '/cmlscratch/sdooley1/data/vggface2/val/'\n",
    "args.demographics_file = '/cmlscratch/sdooley1/data/vggface2/vggface2_demographics.txt'\n",
    "args.RFW_checkpoints_root = 'Checkpoints/vggface2/'\n",
    "args.dataset = 'vggface2'\n",
    "\n",
    "p_images = {\n",
    "    args.groups_to_modify[i]: args.p_images[i]\n",
    "    for i in range(len(args.groups_to_modify))\n",
    "}\n",
    "p_identities = {\n",
    "    args.groups_to_modify[i]: args.p_identities[i]\n",
    "    for i in range(len(args.groups_to_modify))\n",
    "}\n",
    "args.p_images = p_images\n",
    "args.p_identities = p_identities\n",
    "\n",
    "print(\"P identities: {}\".format(args.p_identities))\n",
    "print(\"P images: {}\".format(args.p_images))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataloaders, num_class, demographic_to_labels_train, demographic_to_labels_test = prepare_data(\n",
    "    args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "810f0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['ids','label','gender_expression'])\n",
    "for _, label, gend, ids in dataloaders['test']:\n",
    "    df = df.append(pd.DataFrame({'ids':ids, 'label': label, 'gender_expression': gend}),\n",
    "                  ignore_index=True)\n",
    "    \n",
    "df.to_csv('Checkpoints/vggface2_test_identities_gender.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
